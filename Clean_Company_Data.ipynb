{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1a0e86-79d9-4275-98f0-9640a75f0ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Markus\\AppData\\Local\\Temp\\ipykernel_20680\\392408213.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2d5c6c-657d-4c41-8c88-260e015a5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descriptions = pd.read_csv('data/company/organization_descriptions.csv')\n",
    "df_organizations = pd.read_csv('data/company/organizations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a0b863-97b4-4cf6-afd0-833161b390a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_descriptions[['uuid', 'description']]\n",
    "\n",
    "merged_df = pd.merge(df_organizations, df_subset, on='uuid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232f4284-7933-4969-92d3-e4ef123348c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_tags(category_list):\n",
    "    if pd.isnull(category_list):\n",
    "        return False\n",
    "    tags = category_list.split(',')\n",
    "    for tag in tags:\n",
    "        if tag.lower() in ai_tags:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa80a6f-fdf1-4dc9-bf2c-5f25e710bf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b2b88a53764f209e52467292462e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3487774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filter to companies that only have given tags\n",
    "ai_tags = [\n",
    "    'Artificial Intelligence (AI)', \n",
    "    'Generative AI', \n",
    "    'Intelligent Systems', \n",
    "    'Machine Learning', \n",
    "    'Natural Language Processing', \n",
    "    'Predictive Analytics', \n",
    "    'Robotic Process Automation (RPA)'\n",
    "]\n",
    "#ai_tags = ['Artificial Intelligence (AI)']\n",
    "\n",
    "ai_tags = set([tag.lower() for tag in ai_tags])\n",
    "filtered = merged_df[(merged_df['category_list'].progress_apply(check_for_tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5688cbf-bd48-4241-bf80-5b869728c3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
       "       'updated_at', 'legal_name', 'roles', 'domain', 'homepage_url',\n",
       "       'country_code', 'state_code', 'region', 'city', 'address',\n",
       "       'postal_code', 'status', 'short_description', 'category_list',\n",
       "       'category_groups_list', 'num_funding_rounds', 'total_funding_usd',\n",
       "       'total_funding', 'total_funding_currency_code', 'founded_on',\n",
       "       'last_funding_on', 'closed_on', 'employee_count', 'email', 'phone',\n",
       "       'facebook_url', 'linkedin_url', 'twitter_url', 'logo_url', 'alias1',\n",
       "       'alias2', 'alias3', 'primary_role', 'num_exits', 'description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8869a55-bc47-4d1b-aea6-cb3d98a68abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filtered.drop(columns = ['type', 'permalink', 'cb_url', 'rank','num_funding_rounds', 'total_funding_usd',\n",
    "       'total_funding', 'total_funding_currency_code','employee_count', 'email', 'phone',\n",
    "       'facebook_url', 'linkedin_url', 'twitter_url', 'logo_url', 'alias1',\n",
    "       'alias2', 'alias3','domain','homepage_url','country_code', 'state_code','num_exits','address', 'postal_code','region','last_funding_on'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2706f7c-0d32-4160-8dab-9e3b4ea2981f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf1d25f-df85-45f4-9867-9f14e57a3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "core_count = max(multiprocessing.cpu_count() -2,1)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa9dc22-99a2-4f99-b9df-465ad4721865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that every row has at least one of the descriptions\n",
    "filtered = filtered[filtered['description'].notna() | filtered['short_description'].notna()]\n",
    "#concat into one column\n",
    "filtered['Combined_Text'] = (\n",
    "    filtered['short_description'].fillna('') + '. ' + filtered['description'].fillna('')\n",
    ").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a737e12e-68a4-4936-a1cd-65ad5916d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = filtered['Combined_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2596a2-6487-4e93-878a-fabd396520a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672600adf9b34dbba2f692ca34478592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized_descriptions = []\n",
    "for description in tqdm(nlp.pipe(descriptions, n_process = core_count,batch_size = 64),total=len(descriptions)):\n",
    "    lemmatized_descriptions.append([tok.lemma_ for tok in description])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712400e8-7ecc-4e84-9d0c-ca6a3d1b14e1",
   "metadata": {},
   "source": [
    "# Clean tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7a779a-b40c-4549-b0d8-e61ad0b951ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = set(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75efcbcd-090a-4228-a270-e02b311588d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    two_char = ['ai', 'ip', 'it', 'pc', 'io', 'ar', 'vr', 'ml', 'qc', 'rf', 'ux', 'ui']\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and word != \"-\" and re.match(r'[a-zA-Z-]+$', word)]\n",
    "    filtered_tokens = [word for word in filtered_tokens if len(word) != 1 and (len(word) > 2 or word in two_char)]\n",
    "    return filtered_tokens\n",
    "\n",
    "#there are tokens such as ml- where we only care about the ml part\n",
    "#hyphenated words are also split into two\n",
    "def split_hyphenated_tokens(tokens):\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if \"-\" in token:\n",
    "            split_parts = token.split(\"-\")\n",
    "            processed_tokens.extend([part for part in split_parts if part])\n",
    "        else:\n",
    "            processed_tokens.append(token)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c52273-600c-4e99-8f9c-18d881fae229",
   "metadata": {},
   "outputs": [],
   "source": [
    "dehyphenated_tokens = [split_hyphenated_tokens(tokens) for tokens in lemmatized_descriptions]\n",
    "cleaned_tokens = [clean_tokens(tokens) for tokens in dehyphenated_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33008ce3-6d13-4cd8-b18b-e369c2e08013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to remove low tokens\n",
    "filtered['Lemmatized_Tokens'] = cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d43e0f-1dec-4aed-8000-3968801125e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy list used in patents here\n",
    "bigrams_set = {('time', 'series'),\n",
    " ('sensor', 'datum'),\n",
    " ('computer', 'implement'),\n",
    " ('base', 'station'),\n",
    " ('following', 'step'),\n",
    " ('autonomous', 'driving'),\n",
    " ('information', 'processing'),\n",
    " ('driver', 'assistance'),\n",
    " ('feature', 'extraction'),\n",
    " ('voice', 'command'),\n",
    " ('image', 'datum'),\n",
    " ('method', 'base'),\n",
    " ('augment', 'reality'),\n",
    " ('control', 'method'),\n",
    " ('self', 'adaptive'),\n",
    " ('detection', 'method'),\n",
    " ('readable', 'medium'),\n",
    " ('loss', 'function'),\n",
    " ('follow', 'step'),\n",
    " ('pre', 'train'),\n",
    " ('recognition', 'method'),\n",
    " ('technical', 'field'),\n",
    " ('processor', 'configure'),\n",
    " ('network', 'model'),\n",
    " ('face', 'recognition'),\n",
    " ('aerial', 'vehicle'),\n",
    " ('language', 'processing'),\n",
    " ('attention', 'mechanism'),\n",
    " ('disclose', 'method'),\n",
    " ('medical', 'image'),\n",
    " ('capture', 'image'),\n",
    " ('deep', 'learning'),\n",
    " ('method', 'system'),\n",
    " ('processing', 'unit'),\n",
    " ('threshold', 'value'),\n",
    " ('processing', 'device'),\n",
    " ('control', 'unit'),\n",
    " ('electronic', 'device'),\n",
    " ('random', 'forest'),\n",
    " ('human', 'body'),\n",
    " ('base', 'deep'),\n",
    " ('video', 'frame'),\n",
    " ('motor', 'vehicle'),\n",
    " ('artificial', 'intelligence'),\n",
    " ('computer', 'program'),\n",
    " ('readable', 'storage'),\n",
    " ('generative', 'adversarial'),\n",
    " ('obstacle', 'avoidance'),\n",
    " ('identification', 'method'),\n",
    " ('model', 'train'),\n",
    " ('license', 'plate'),\n",
    " ('prior', 'art'),\n",
    " ('large', 'scale'),\n",
    " ('particle', 'swarm'),\n",
    " ('computer', 'vision'),\n",
    " ('belong', 'technical'),\n",
    " ('control', 'system'),\n",
    " ('transitory', 'computer'),\n",
    " ('energy', 'consumption'),\n",
    " ('learn', 'model'),\n",
    " ('region', 'interest'),\n",
    " ('problem', 'solve'),\n",
    " ('voice', 'recognition'),\n",
    " ('natural', 'language'),\n",
    " ('early', 'warning'),\n",
    " ('power', 'consumption'),\n",
    " ('support', 'vector'),\n",
    " ('processing', 'method'),\n",
    " ('wireless', 'communication'),\n",
    " ('bounding', 'box'),\n",
    " ('image', 'processing'),\n",
    " ('train', 'neural'),\n",
    " ('big', 'datum'),\n",
    " ('audio', 'signal'),\n",
    " ('user', 'terminal'),\n",
    " ('neural', 'network'),\n",
    " ('feature', 'point'),\n",
    " ('processing', 'apparatus'),\n",
    " ('anomaly', 'detection'),\n",
    " ('vector', 'machine'),\n",
    " ('power', 'supply'),\n",
    " ('intelligence', 'ai'),\n",
    " ('prediction', 'model'),\n",
    " ('content', 'item'),\n",
    " ('method', 'far'),\n",
    " ('model', 'obtain'),\n",
    " ('autonomous', 'vehicle'),\n",
    " ('feature', 'map'),\n",
    " ('comprise', 'step'),\n",
    " ('magnetic', 'resonance'),\n",
    " ('speech', 'recognition'),\n",
    " ('main', 'body'),\n",
    " ('objective', 'function'),\n",
    " ('train', 'machine'),\n",
    " ('artificial', 'neural'),\n",
    " ('recognition', 'result'),\n",
    " ('image', 'capture'),\n",
    " ('real', 'time'),\n",
    " ('nucleic', 'acid'),\n",
    " ('mobile', 'terminal'),\n",
    " ('face', 'image'),\n",
    " ('drawing', 'fig'),\n",
    " ('storage', 'medium'),\n",
    " ('learning', 'algorithm'),\n",
    " ('relate', 'technical'),\n",
    " ('non', 'transitory'),\n",
    " ('fuel', 'cell'),\n",
    " ('program', 'product'),\n",
    " ('embodiment', 'invention'),\n",
    " ('compute', 'device'),\n",
    " ('character', 'string'),\n",
    " ('mobile', 'device'),\n",
    " ('path', 'planning'),\n",
    " ('management', 'system'),\n",
    " ('unmanned', 'aerial'),\n",
    " ('acquisition', 'unit'),\n",
    " ('apparatus', 'method'),\n",
    " ('extract', 'feature'),\n",
    " ('datum', 'set'),\n",
    " ('device', 'storage'),\n",
    " ('fuzzy', 'logic'),\n",
    " ('short', 'term'),\n",
    " ('fault', 'diagnosis'),\n",
    " ('genetic', 'algorithm'),\n",
    " ('convolutional', 'neural'),\n",
    " ('human', 'face'),\n",
    " ('improve', 'accuracy'),\n",
    " ('equipment', 'storage'),\n",
    " ('light', 'source'),\n",
    " ('user', 'interface'),\n",
    " ('time', 'period'),\n",
    " ('target', 'object'),\n",
    " ('drawing', 'figure'),\n",
    " ('point', 'cloud'),\n",
    " ('super', 'resolution'),\n",
    " ('deep', 'neural'),\n",
    " ('wind', 'turbine'),\n",
    " ('decision', 'tree'),\n",
    " ('determination', 'unit'),\n",
    " ('system', 'method'),\n",
    " ('feature', 'vector'),\n",
    " ('web', 'page'),\n",
    " ('electronic', 'equipment'),\n",
    " ('unit', 'configure'),\n",
    " ('computer', 'readable'),\n",
    " ('recording', 'medium'),\n",
    " ('machine', 'learning'),\n",
    " ('high', 'resolution'),\n",
    " ('key', 'point'),\n",
    " ('method', 'device'),\n",
    " ('training', 'datum'),\n",
    " ('mobile', 'robot'),\n",
    " ('learning', 'model'),\n",
    " ('air', 'conditioner'),\n",
    " ('recurrent', 'neural'),\n",
    " ('knowledge', 'graph'),\n",
    " ('machine', 'learn'),\n",
    " ('reinforcement', 'learning'),\n",
    " ('virtual', 'reality'),\n",
    " ('control', 'device'),\n",
    " ('remote', 'sense'),\n",
    " ('blood', 'vessel'),\n",
    " ('internet', 'thing'),\n",
    " ('system', 'comprise'),\n",
    " ('training', 'sample'),\n",
    " ('parking', 'space'),\n",
    " ('input', 'datum'),\n",
    " ('training', 'set')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a838dc46-7be2-4a06-88a3-24426d070d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bigrams(text, bigrams_set):\n",
    "    combined_text = []\n",
    "    i = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    skip = False\n",
    "    while i < text_len:\n",
    "        if i < text_len - 1 and (text[i], text[i + 1]) in bigrams_set:\n",
    "            combined_text.append(f\"{text[i]}_{text[i + 1]}\")\n",
    "            i += 1\n",
    "            skip = True\n",
    "        else:\n",
    "            if skip == True:\n",
    "                i+=1\n",
    "                skip = False\n",
    "            else:\n",
    "                combined_text.append(text[i])\n",
    "                i += 1\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd6258de-b4a9-4e2b-b535-448caf5b3e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a77f3a645e4f79a4b53653a041c7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered['Bigrams_Tokens'] = filtered['Lemmatized_Tokens'].progress_apply(lambda x: detect_bigrams(x, bigrams_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02d78011-1704-45a6-946a-37da93aa72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['founded'] = filtered['founded_on'].fillna(filtered['created_at']).astype(str).str[:4]\n",
    "filtered.to_csv('data/company/Cleaned_Companies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d6632-be34-45fb-acc2-ebdccb88fdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
